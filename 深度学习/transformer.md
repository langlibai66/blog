# transformer

transformer架构

![../_images/transformer.svg](https://zh-v2.d2l.ai/_images/transformer.svg)

transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层

第一个子层是多头自注意力汇聚，第二个子层是基于位置的前馈网络

收到残差网络的启发，每个子层都采用了残差连接

transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层中插入了第三个子层，成为编码器-解码器注意力层，

## 多头注意力

![../_images/multi-head-attention.svg](https://zh-v2.d2l.ai/_images/multi-head-attention.svg)

多头注意力是一种特殊的使用自注意力的结构

是说同一k,v,q，希望抽取不同的信息，例如短距离关系和长距离关系

多头注意力使用h个独立的注意力池化，合并各个头输出得到最后的输出

##  有掩码的多头注意力

训练解码器对于序列中一个元素输出时，不应该考虑该元素之后的元素，可以通过掩码来实现，也就是计算$X_i$输出时，假装当前序列长度为$i$

##  基于位置的前馈网络

也就是图中的逐位前馈网络

实际上就是全连接，batch_size,n—》序列长度,dimension

由于n的长度不是固定的

- 将输入形状由(b,n,d)变换成(bn,d)
- 作用两个全连接层
- 输出形状由(bn,d)变换回(b,n,d)
- 等价于两层核窗口为1的一维卷积层

## 层归一化

