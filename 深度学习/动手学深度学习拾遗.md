# 动手学深度学习拾遗

##### 学习完一遍后，通过基于(波士顿房价预测，叶子分类)实战将已经快遗忘的知识点进行拾遗复习

## softmax

首先是softmax分类头的问题，softmax作为分类问题中常见的分类方法，在pytorch中并未直接实现，而是附加到了交叉熵损失函数中，所以在使用时直接进行使用就可以了

## 正则化

缓解过拟合问题，一是我们可以采集更多的优质数据来缓解，在数据已经尽可能多且优质时，就要将重心放在正则化上

限制特征数量是缓解过拟合的一种常见技术，但是简单丢弃特征这一工作过于生硬，我们模型复杂度的增加需要一个更细粒度的工具

最常用的方法是将范数作为惩罚项加入到损失函数的最小化中一同最小化，当权重函数增长过大时就会更集中于最小化权重函数

在简洁实现中只需要将optim的weight_decay参数指定出来就可以实现

## batch_normal

batchnorm能够解决梯度消失问题，将每个batch的数据拿出来做一次正则化，减少内部的协变量偏移（一种直觉，并非严谨协变量偏移）

在卷积（全连接）层输出后，激活函数使用前使用

torch中使用为torch.nn.BatchNormal(first_dim)

