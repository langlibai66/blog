# 线性回归

首先是书上基础的概念

它在回归的各种标准工具中最简单而且最流行。 线性回归基于几个简单的假设： 首先，假设自变量x和因变量y之间的关系是线性的， 即y可以表示为x中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

## 线性模型

线性模型很简单，就是加权和，是各个参数经过加权后再加上偏置得到的值，主要是对于参数与权重的确定，而偏置是对其准确性的调整

## 损失函数

*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。

## 解析解

线性回归刚好是一个很简单的优化问题。 与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。

![微信图片_20230330161247.jpg](https://beyondclouds.oss-cn-beijing.aliyuncs.com/blog/images/7ca809bb-07f7-4d32-9b61-f45cf5b08592.jpg)

![微信图片_20230330142050.jpg](https://beyondclouds.oss-cn-beijing.aliyuncs.com/blog/images/56dc8470-0b0b-4b4c-91b6-989cdd598609.jpg)



像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

## 随机梯度下降

解析解并不好寻找，我们用到一种名为*梯度下降*（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做*小批量随机梯度下降*。

然后用已经学习的线性回归模型就能对目标进行预测。

## 向量化加速

在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本（如利用小批量样本实现随机梯度下降）。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

## 正态分布与平方损失

正态分布与线性回归密切相关，本次研究针对于对于噪声分布的假设，服从正态分布的噪声很理想化，均值为0，似然就是对于预测值的接近，最大似然值就是对于预测值的估计

![uTools_1680163627216.png](https://beyondclouds.oss-cn-beijing.aliyuncs.com/blog/images/b104c3f8-1943-4271-a94c-418b924eba19.png)

# 反向传播

反向传播依靠计算图实现，见深度学习入门鱼书p146

理论知识到这里

