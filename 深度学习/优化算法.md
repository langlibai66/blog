# 优化算法小记

内容很多，见d2l书上的对应章节

不过想要记录的就只有

## SGD加上moment选项

所谓moment选项就是使用冲量法将梯度变平滑，使得平滑过的梯度对权重进行更新，在一定程度上能够加快收敛（不符合直觉，但是有效，因为平滑所以后续所需要的步数更少了），使用时就是SGD的优化器加上moment选项即可

## Adam

Adam是一个简单的优化器，使用了更多的平滑，使用效果就是对于学习率不再敏感，不需要怎么调整参数就可以达到很好的效果

