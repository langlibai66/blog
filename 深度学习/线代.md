# 线性代数

##### 数学知识

##### 之前已经了解了表示标量向量矩阵的方法,接下来不算总结，仅为摘要书上内容

## 线性代数计算的实现

### 1.矩阵转置

#### .t

例子:

```python
A = torch.arange(20).reshape(5, 4)
A
```

```
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]])
```

```python
A.T
```

```
tensor([[ 0,  4,  8, 12, 16],
        [ 1,  5,  9, 13, 17],
        [ 2,  6, 10, 14, 18],
        [ 3,  7, 11, 15, 19]])
```

### 2.降维

#### .sum

对于向量，降维就是访问sum

```
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```

```
(tensor([0., 1., 2., 3.]), tensor(6.))
```

默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。

```
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```

```
(tensor([40., 45., 50., 55.]), torch.Size([4]))
```

指定`axis=1`将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。

```
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```

```
(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))
```

### 3.平均值

#### .mean

```
A.mean()
```

```
(tensor(9.5000), tensor(9.5000))
```

### 4.非降维求和

```
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```

```
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```

#### .cumsum

```
A.cumsum(axis=0)
```

```
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
```

### 5.点积

采用按元素乘的和即可

```
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```

```
(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))
```

```
torch.sum(x * y)
```

```
tensor(6.)
```

### 6.矩阵-向量积

#### .mv

#### 在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。 当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积。 注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。

```
A.shape, x.shape, torch.mv(A, x)
```

```
(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))
```

### 7.矩阵-矩阵乘法

#### .mm

```
B = torch.ones(4, 3)
torch.mm(A, B)
```

```
tensor([[ 6.,  6.,  6.],
        [22., 22., 22.],
        [38., 38., 38.],
        [54., 54., 54.],
        [70., 70., 70.]])
```

矩阵-矩阵乘法可以简单地称为**矩阵乘法**，不应与”Hadamard积(就是同位置简单相乘)”混淆。

### 8.范式

#### 线性代数中最有用的一些运算符是*范数*（norm）。 非正式地说，向量的*范数*是表示一个向量有多大。 这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。

![uTools_1679494004613.png](https://beyondclouds.oss-cn-beijing.aliyuncs.com/blog/images/8ccb7f69-5d8c-424a-a674-edd74989196b.png)![uTools_1679494014794.png](https://beyondclouds.oss-cn-beijing.aliyuncs.com/blog/images/69341618-f121-42c9-8462-6b4d2bd64e42.png)![uTools_1679494023950.png](https://beyondclouds.oss-cn-beijing.aliyuncs.com/blog/images/60a7762c-1500-4fb9-8041-59c5b995f462.png)



#### .abs().sum()

L1范式的使用

```
torch.abs(u).sum()
```

```
tensor(7.)
```

#### .norm

L2范式的使用

```
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```

```
tensor(5.)
```