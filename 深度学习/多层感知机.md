# 多层感知机

##### 前面咱们使用过单层的感知机了，多层感知机就是在原有的基础上，加入隐藏层，从而克服线性隐藏层的限制，要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 通常缩写为MLP。要实现

## 数值稳定性

##### 数值稳定性在深度学习中是十分重要的

##### 在进行反向传播求梯度时，根据链式法则，我们知道，梯度计算的结果是有许多的矩阵与一个梯度向量的乘积，会受到数值下溢的影响，引起梯度爆炸或梯度消失，原理很简单，就是过多的概率相乘带来的结果，不稳定的梯度带来的风险很大

### 梯度消失

参数更新过小，导致模型无法学习

sigmoid函数就是导致梯度消失的常见原因，由于sigmoid函数是饱和函数，在输入很大或很小时其梯度都会消失。导致模型梯度被切断

### 梯度爆炸

参数更新过大，破坏了模型的稳定收敛

与模型消失相反，但同样让人烦恼，模型爆炸也是一种不可避免的问题

### 对称性

神经网络设计中的另一个问题是其参数化所固有的对称性。

在这种情况下，我们可以对第一层的权重进行重排列， 并且同样对输出层的权重进行重排列，可以获得相同的函数。

在基于梯度的迭代（例如，小批量随机梯度下降）之后， W1的所有元素仍然采用相同的值。 这样的迭代永远不会打破对称性，我们可能永远也无法实现网络的表达能力。 隐藏层的行为就好像只有一个单元。 请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。

## 模型初始化

在模型训练中，我们想努力使训练更稳定，目标就是要让梯度值在合理的范围内。

可使用的方法有：

- 把乘法变加法
- 归一化
- 合理的权重初始和激活函数

### 权重初始化

- 在合理值区间里随机初始参数
- 训练开始的时候更容易有数值不稳定
  - 远离最优解的地方损失函数表面可能很复杂
  - 最优解附近表面会比较平

### Xavier初始

不能同时满足前一层与后一层的方差＝1，采用折中的办法

这节，我学不懂，等以后会概率论了再说吧

##### 只听懂了一点就是激活函数的由来

##### 假设我有一个线性函数等于ax+b，我要实现通过这个函数，让我的均值与方差保持不变，可以求解得到a=1,b=0。由此达到relu函数为什么好用了。

#### 检查激活函数

使用泰勒展开检查 可以发现各个激活函数的合理性

## 环境与分布偏移

模型的数据来源，数据精度是很重要的问题，我们在训练模型的时候一定关注这些问题，当数据分布改变时，模型部署可能会出现灾难性的失败。

### 分布偏移类型













