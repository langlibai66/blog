# ResNet细读与实现

## 出现缘由

在resnet出现前，Alexnet的成绩让大家认为神经网络在图像上要达到更好的效果，就要让网络越来越深，网络的深度一定能带来更好的成果，让大家深信不疑的一大原因就是大家认为即使已经拟合出了非常好的函数，更深的网络只需要持续进行恒等变换，也就是y=x就好了，这样就能使得结果再提升的过程中不会出现任何问题

但是resnet的团队否认了这一点，他们发现随着网络层数的不断加深，最终的效果可能会变差，resnet团队把这个现象称为退化，之所以出现退化，主要是因为深层神经网络难以实现恒等变换

众所周知深度学习相比于机器学习的特点除了自动学习，网络层数深，非线性转化（激活函数），深度学习在网络层数变深后，使用的非线性转化也就是特征函数越来越多，非线性转化主要是可以将数据映射到更高维的空间，让它们可以拟合任意的函数以更好的实现数据分类，而非线性转化的使用次数过多后，就会导致数据被映射的空间越来越离散，难以让数据回到原点，或者说，神经网络将这些数据映射回原点所需要的计算量，已经远远超过我们所能承受的。

所以我们要让实现非线性转化的时候顾及线性分支，在线性与非线性中找到一个平衡。

