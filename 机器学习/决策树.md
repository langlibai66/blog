# 决策树

## 简介

决策树是一树状结构，它的每一个叶节点对应着一个分类，非叶节点对应着在某个属性上的划分，根据样本在该属性上的不同取值将其划分成若干个子集。对于非纯的叶节点，多数类的标号给出到达这个节点的样本所属的类。构造决策树的核心问题是在每一步如何选择适当的属性对样本做拆分。

对一个分类问题，从已知类标记的训练样本中学习并构造出决策树的过程就是决策树

决策树的典型算法有三种：ID3，C4.5，CART

## 信息增益

我们希望的是决策树进行分类后分支节点所包含的样本尽可能真正的属于同一类别，也就是纯度够高

信息熵是度量样本集合纯度的一个重要指标

![img](https://pic3.zhimg.com/80/v2-84d90b3b2a2ed662667c879344fdca6a_1440w.webp)

信息熵即为不确定性，信息熵越小则纯度越高

信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度

**如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即：**

h(x,y) = h(x) + h(y)

由于x，y是俩个不相关的事件，那么满足p(x,y) = p(x)*p(y).

根据上面推导，**我们很容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）**。因此我们有信息量公式如下：

![h(x)=-log_{2}p(x)](https://www.zhihu.com/equation?tex=h%28x%29%3D-log_%7B2%7Dp%28x%29+)

由此熵出来了

### 增益率

由于信息增益并不一定能挑选出最佳结点，例如选择序号作为分类属性，纯度将会很高，因为每个种类下都只有一个样本

所以为了防止这种情况我们引入信息增益，信息增益是（补一个公式）

关于属性a固定的信息熵，是说a这个属性本身就能分出来多少的类别，我们想要的就是增益比较大的

所以在C4.5分类树就是采用先选取高于平均水平的属性，然后再选择增益率大的，以保证选取属性的合理性

也就是说要选择更加值得的分支，而不是目前来看效果好的，目的是为了将所有特征最大化，对于增益不高的属性没有必要使用

## ID3

在决策树的各级节点上使用信息增益作为属性的选择标准来帮助确定生成属性



## C4.5

## CART