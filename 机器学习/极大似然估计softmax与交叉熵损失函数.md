# 极大似然估计与交叉熵损失函数

很久之前就已经似懂非懂的解决过这个问题了，但是发现还是存在疑虑

## 极大似然估计

### 概率

特定环境下某件事情发生的可能性

可以通过参数对概率进行预测

只有在发生之前是有意义的

### 似然

根据结果来推测过程中的某些参数

基于结果对于具体参数进行估计

##### 极大似然估计简单是一种通过观察结果来求条件的方式

同时也是通过结果来求参数的办法
$$
概率：P(x|\theta) \\
似然：L(\theta|x)
$$

## softmax

用于将各个可能的值映射到[0,1]之间，并且使得所有可能的值的和为1

字面上来说，softmax分为soft与max，与此相对的就是hardmax，所谓的hardmax就是直接找出最大值，忽略掉其他的值，我们想要的是软max，对于每个类别都要有一定的置信度
$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$
求出其最大值就是目标类别

使用指数函数有其一定的优点，但也存在缺点

优点是在数值较大时斜率会越来越大，由此拉开距离，但缺点就是指数的值容易过大导致溢出

当然针对数值溢出有其对应的优化方法，将每一个输出值减去输出值中最大的值。

也就是exp($o_j-max(o)$)，虽然变成负值，但是指数函数的性质让分子分母依然是负数并且依旧有效

一般softmax分类头将会配合交叉熵损失函数，他们俩的精巧设计会让对于loss函数的求导非常简单

## 交叉熵

##### 主要用于度量同一个随机变量X的预测分布Q与真实分布P之间的差距，反映的是困难程度

我们使用极大似然估计法，求对数似然来作为损失函数
$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$
这里先假设真实结果为1，其余均为0

所以在分类问题中只需要将对数前乘上值即可，乘以对应值的概率以求得损失

最后得出损失函数
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.
$$
这个损失函数通常被称为*交叉熵损失*，能够表示出不确定性程度，比较直观的反应分类损失，使用交叉熵还因为它有一个特性是其他损失函数不容易替代的，就是交叉熵更强烈的惩罚错误的输出。如果有非常错误的输出，它的值就会变化很大，反馈很强，并且导数更大。