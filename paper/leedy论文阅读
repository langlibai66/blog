# MPT文章阅读笔记
Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking
一文中讲述了STNet实现的过程与原理
## 主要内容
模型是基于多模态的模型，使用多模态融合式提高说话人跟踪准确性与鲁棒性的有效方法，在复杂场景中，如何利用多模态信息的互补性是挑战性问题
说话人跟踪是智能系统实现行为分析与人机交互的基础任务，为了提高追踪器的准确性，利用多模态传感器捕获更丰富的信息，其中视觉与听觉作为人类理解周围环境和与他人互动的主要感官，利用试听信号的互补性能够提供必要的追踪线索，在目标遮挡，视图有限，光线变化与房间混响等复杂情况下都能实现准确的追踪

本文提出了一种新型多模态感知追踪器，用于使用音频与视觉模态进行说话人跟踪：
### stGCF
STGCF是时空全局相干场，我们使用基于stGCF的新型声学图用于异构信号的融合
其使用相机模型将音频线索映射到与视觉线索一致的定位空间
### 多模态感知注意网络
用于推导出测量噪声干扰间歇性音频和视频流的可靠性与有效性的感知权重
### 自监督学习 
本文提出了一种特殊的跨模态自监督学习方法，通过利用不同模态之间的互补性与一致性来模拟音频与视觉观察的置信度

神经网络很少被用在追踪是因为分类中的正样本只是初始帧中的随机状态，高性能分类器中的数据量太少

使用注意力网络	训练中间感知组件可以提供新的思路与想法	

还有一个问题，也是我们需要解决的问题就是 音视频数据的异构性导致网络初期很难做到统一

所以我们此时引入时空全局相干场 --->stGCF图

## stGCF

为了将音频信息映射到空间中，我们使用相机模型将音频线索投影到图像特征空间

为了生成融合图 我们集成的视听线索通过网络估计的感知权重进行加权  最后采用融合图改进的基于PF的追踪器来确保多模态观测的平滑

![image-20240501233029421](https://gitee.com/ai-yang-chenxu/img/raw/master/img/image-20240501233029421.png)